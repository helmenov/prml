{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-KmrX-7HqF9d",
        "UW1hl94VG9Z8"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsakailab/prml/blob/master/ipynb/ex_Perceptrons.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# パーセプトロンの動作について学ぶ（Behavior of perceptrons）\n",
        "----\n",
        "\n",
        "氏名：\n",
        "\n",
        "学生番号：\n",
        "\n",
        "----\n",
        "[A Neural Network Playground](http://playground.tensorflow.org/)を用いて，パーセプトロンの振る舞い考察しましょう．\n",
        "Playground詳しい説明は[ここにあります](https://cloud.google.com/blog/products/ai-machine-learning/understanding-neural-networks-with-tensorflow-playground)．\n",
        "\n",
        "以下の課題に取り組む際の注意です．\n",
        "- \"Run/Pause\"の左のボタン\"Reset the network\"を用いて反復試行すること．必ず同じモデルに収束するとは限りません．\n",
        "- 左下の\"[REGENERATE]\"で再生成したデータも試すこと．特定のデータについてではなく，普遍的に言える事柄を見つけましょう．\n",
        "- 十分収束するまで観察すること．学習中のモデルよりも必ず良いモデルに収束するのでしょうか．\n",
        "- 右下の\"Show test data\"で表示されるデータ（訓練データ以外のデータ）に対しても識別できているかどうかで評価すること．\n",
        "\n",
        "特に，訓練データが少なく，ノイズの影響も無視できない問題設定にすると，考察に有益です．  \n",
        "Ratio of training to test data: 20%，Noise: 25 の設定で実験します．\n",
        "\n",
        "(a) [モデル：単純パーセプトロン，　データセット：\"Gaussian\"](https://playground.tensorflow.org/#activation=sigmoid&batchSize=10&dataset=gauss&regDataset=reg-plane&learningRate=0.03&regularizationRate=0.01&noise=30&networkShape=&seed=0.85984&showTestData=false&discretize=true&percTrainData=20&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&batchSize_hide=false)\n",
        "\n",
        "(b) [モデル：3層パーセプトロン，　データセット：\"Circle\"](https://playground.tensorflow.org/#activation=sigmoid&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.3&regularizationRate=0.01&noise=25&networkShape=1&seed=0.28240&showTestData=false&discretize=true&percTrainData=20&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&batchSize_hide=false)\n",
        "\n",
        "(c) [モデル：単純パーセプトロン，データセット：\"Circle\"](https://playground.tensorflow.org/#activation=sigmoid&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.3&regularizationRate=0.01&noise=25&networkShape=&seed=0.07326&showTestData=false&discretize=true&percTrainData=20&x=true&y=true&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=true&cosY=false&sinY=true&collectStats=false&problem=classification&initZero=false&hideText=false&batchSize_hide=false)\n",
        "\n",
        "まず，(a)で，単純パーセプトロンが線形識別器であることを確認してから，課題に着手しましょう．"
      ],
      "metadata": {
        "id": "b_Ll73T9wjPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## 基本課題（必須）\n",
        "\n",
        "    1. (b)では，中間層に何個以上のノード（neurons）が必要ですか．その理由を述べてください（結果論は不可です）．\n",
        "      また，多数のノード（例えば8個）で学習を続けると，どのような問題が生じますか．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "\n",
        "    2. (c)のモデルは単純パーセプトロンなのに，決定境界が直線でないのはなぜですか．\n",
        "      また，入力に使われているどの特徴が強く使われる傾向がありますか．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "\n",
        "    3. 正則化（regularization）を\"L1\"に設定すると，モデルに特徴を選択させる学習の効果があります．この効果を(c)の例で具体的に説明してください．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "\n",
        "    4.その他，気づいたこと，調べたことがあれば書いてください．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-KmrX-7HqF9d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW1hl94VG9Z8"
      },
      "source": [
        "----\n",
        "## 発展課題（任意）\n",
        "\n",
        "Playgroundの右上に表示される \"Test loss\"，\"Training loss\" について考えましょう．\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPcFoZwoU2fp",
        "cellView": "form"
      },
      "source": [
        "#@title 例題用のデータを生成します．\n",
        "import numpy as np\n",
        "npos = 30\n",
        "nneg = 30\n",
        "np.random.seed(321)\n",
        "X = np.r_[np.random.randn(npos, 2) + [3, 3], np.random.randn(nneg, 2)]\n",
        "# [1,1,...,1,-1,-1,...,-1]\n",
        "y = np.array([1] * npos + [-1] * nneg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title グラフを描く関数を定義します．\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import TwoSlopeNorm as tsn\n",
        "\n",
        "def plot2d_classification(decision_function, X_train, y_train, X_test=None, y_test=None, dx=0.02, cmap=plt.cm.bwr, xlim=None, ylim=None, levels=None, colors='k', bins=None):\n",
        "\n",
        "    fig, axes = plt.subplots(figsize=(12,8), nrows=1, ncols=2)\n",
        "\n",
        "    ax = axes[0]\n",
        "    if xlim is None:\n",
        "        xlim = [X_train[:, 0].min() - .5, X_train[:, 0].max() + .5]\n",
        "    if ylim is None:\n",
        "        ylim = [X_train[:, 1].min() - .5, X_train[:, 1].max() + .5]\n",
        "\n",
        "    xx, yy = np.meshgrid(np.arange(xlim[0], xlim[1], dx), np.arange(ylim[0], ylim[1], dx))    \n",
        "\n",
        "    # Show prediction (P(y=+1 | X) by color by assigning a color to each point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "    Z = decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    # Put the result into a color plot\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    norm = tsn(vmin=np.minimum(Z[:].min(),-1e-6), vcenter=0, vmax=np.maximum(Z[:].max(),1e-6))\n",
        "    ax.pcolor(xx, yy, Z, cmap=cmap, alpha=0.1, edgecolors=None, norm=norm)\n",
        "    if levels is not None:\n",
        "        lvls = list(levels.keys())\n",
        "        linestyles = list(levels.values())\n",
        "        ax.contour(xx, yy, Z, levels=lvls, colors=colors, linestyles=linestyles, alpha=0.5)\n",
        "\n",
        "    # Plot the training points\n",
        "    ax.scatter(X_train[y_train>0, 0], X_train[y_train>0, 1], c='r',  marker='s', edgecolors='k', label='Training data', alpha=1)\n",
        "    ax.scatter(X_train[y_train<=0, 0], X_train[y_train<=0, 1], c='b', marker='o', edgecolors='k', label='Training data', alpha=1)\n",
        "        \n",
        "    # and testing points if given\n",
        "    if X_test is not None and y_test is not None:\n",
        "        ax.scatter(X_test[y_test>0, 0], X_test[y_test>0, 1], c='k',  marker='s', edgecolors='k', label='Test data', alpha=1)\n",
        "        ax.scatter(X_test[y_test<=0, 0], X_test[y_test<=0, 1], c='k', marker='o', edgecolors='k', label='Test data', alpha=1)\n",
        "        plt.legend(loc=\"upper right\", fontsize=16, frameon=True)\n",
        "        ax.get_legend().legendHandles[2].set_color('k')\n",
        "        ax.get_legend().legendHandles[3].set_color('k')\n",
        "\n",
        "    ax.set_xlim(xx.min(), xx.max())\n",
        "    ax.set_ylim(yy.min(), yy.max())\n",
        "    plt.axis('tight')\n",
        "    plt.xlabel('x1', fontsize=16)\n",
        "    plt.ylabel('x2', fontsize=16)\n",
        "    plt.xticks(fontsize=16)\n",
        "    plt.yticks(fontsize=16)\n",
        "    ax.set_aspect('equal')\n",
        "    #plt.gca().set_aspect('equal')\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "    if bins is None:\n",
        "        bins = len(y_train) // 4\n",
        "\n",
        "    ax = axes[1]\n",
        "    pred = decision_function(X_train)\n",
        "    plt.hist( [ pred[y_train>0], pred[y_train<=0] ], bins=bins, histtype='stepfilled', density=False, alpha=0.5, color=['r', 'b'], label=['$y=+1$', '$y=-1$'])\n",
        "    if X_test is not None and y_test is not None:\n",
        "        pred = decision_function(X_test)\n",
        "        plt.hist( [ pred[y_test>0], pred[y_test<=0] ], bins=bins, histtype='stepfilled', density=False, alpha=0.3, color=['r', 'b'], label=['$y_{test}=+1$', '$y_{test}=-1$'])\n",
        "    plt.xlabel(\"$g(x)$\", fontsize=16)\n",
        "    plt.ylabel(\"Frequency\", fontsize=16)\n",
        "    plt.legend(loc=\"upper right\", fontsize=16, frameon=True)\n",
        "    plt.axis('tight')\n",
        "    plt.xticks(fontsize=16)\n",
        "    plt.yticks(fontsize=16)\n",
        "    from matplotlib.ticker import FormatStrFormatter\n",
        "    #plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%1.0f'))\n",
        "    ax.yaxis.set_major_formatter(FormatStrFormatter('%1.0f'))\n",
        "    ax.set_aspect(0.8)\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VvmOmGO2jR-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 損失関数を定義します．"
      ],
      "metadata": {
        "id": "bSLGHQlGrBb6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJxRr0_4G9aI"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def LogisticLoss(y, g):\n",
        "    return np.mean(np.log(1 + np.exp(-y*g)))\n",
        "\n",
        "def HingeLoss(y, g):\n",
        "    return np.mean(np.maximum(1.0 - y*g, 0))\n",
        "\n",
        "# 線形識別関数\n",
        "def g(X, w):\n",
        "    return w[0] + np.dot(X, w[1:])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $[w_0, w_1, w_2]^\\top$はパーセプトロンの重みです．表示される損失関数の値がなるべく小さくなるように調整してください．\n",
        "    1. どのようなとき，損失が大きく・小さくなりますか．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "    2. w0, w1, w2 によって決定境界はどのように変わりますか．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7FUjbZ2jrXbX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIQAY-CgG9ah",
        "cellView": "form"
      },
      "source": [
        "from sklearn.linear_model import Hinge\n",
        "#@title {run: \"auto\"}\n",
        "w0 = 0.1 #@param{type:\"slider\", min:-5, max:5, step:0.1}\n",
        "w1 = -1 #@param{type:\"slider\", min:-5, max:5, step:0.1}\n",
        "w2 = -3 #@param{type:\"slider\", min:-5, max:5, step:0.1}\n",
        "w = np.array( [w0, w1, w2] )\n",
        "\n",
        "ll = LogisticLoss(y, g(X, w))\n",
        "hl = HingeLoss(y, g(X, w))\n",
        "print(\"Logistic loss = %f,  Hinge loss =%f\" % (ll, hl))\n",
        "plot2d_classification(lambda X: g(X, w), X, y, levels={0.0:'-'}, dx=0.2)\n",
        "#histogram_predict(lambda X: g(X, w), X, y)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}