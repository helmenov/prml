{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnd2d_ex1_LinearDiscriminantAnalysis_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsakailab/prml/blob/master/ipynb/rnd2d_ex1_LinearDiscriminantAnalysis_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Xf5xoadG9Z0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW1hl94VG9Z8",
        "colab_type": "text"
      },
      "source": [
        "Classification by linear discriminant analysis of 2D data set\n",
        "==================="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgHEfe14h1eb",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Define a plot function\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from matplotlib import pyplot as plt\n",
        "# Visualization of the estimated distributions\n",
        "def plot2d_LDA(model, X_train, y_train, X_test=None, y_test=None, cmap=None, xlim=None, ylim=None, dxlim=0.5, dylim=0.5, levels=None, linestyles=None, markers=None, colors=None):\n",
        "\n",
        "    plt.figure()\n",
        "    ax = plt.axes()\n",
        "\n",
        "    if xlim is None:\n",
        "        xlim = [X_train[:, 0].min() - dxlim, X_train[:, 0].max() + dxlim]\n",
        "    if ylim is None:\n",
        "        ylim = [X_train[:, 1].min() - dylim, X_train[:, 1].max() + dylim]\n",
        "\n",
        "    xg, yg = np.arange(xlim[0], xlim[1], (xlim[1]-xlim[0])/300.), np.arange(ylim[0], ylim[1], (ylim[1]-ylim[0])/300.)\n",
        "    xx, yy = np.meshgrid(xg, yg)    \n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = ['Blues', 'Reds', 'Greens', 'BuPu', 'RdPu', 'YlGn']\n",
        "    if markers is None:\n",
        "        markers = ['o', 's', '^', '*', '+', 'x']\n",
        "    if colors is None:\n",
        "        colors = ['b', 'r', 'g', 'c', 'm', 'y']\n",
        "    ncmap, nm, nc = len(cmap), len(markers), len(colors)\n",
        "\n",
        "    ncls = len(model.classes_)\n",
        "    vmax = 1.  / (2. * np.pi * np.sqrt(np.linalg.det(model.covariance_)))\n",
        "    for k in range(ncls):\n",
        "        # Bivariate normal joint density\n",
        "        Z = stats.multivariate_normal(model.means_[k,:], model.covariance_).pdf(np.dstack((xx,yy)))\n",
        "\n",
        "        # Put the result into a color plot\n",
        "        #ax.pcolor(xx, yy, Z, cmap=cmap[k], alpha=0.5, edgecolors=None)\n",
        "        Zm = np.ma.masked_array(Z, Z < 0.03*vmax)\n",
        "        ax.pcolorfast(xg, yg, Zm.reshape(xx.shape), cmap=cmap[k%ncmap], alpha=0.5)\n",
        "        if levels is not None:\n",
        "            ax.contour(xx, yy, Z.reshape(xx.shape), levels=levels, colors='k', linestyles=linestyles, alpha=0.2)\n",
        "        else:\n",
        "            levels = np.arange(0, vmax, vmax/8.)\n",
        "            ax.contour(xx, yy, Z.reshape(xx.shape), levels=levels, colors='k', linestyles=linestyles, alpha=0.2)\n",
        "\n",
        "    if ncls == 2:\n",
        "        decision_function = lambda X: np.log(np.array(model.predict_proba(X)[:,1])/np.array(model.predict_proba(X)[:,0]))\n",
        "        Z = decision_function(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "        ax.contour(xx, yy, Z.reshape(xx.shape), levels=[0.], colors='k', linestyles=['--'], alpha=1)\n",
        "\n",
        "    # Plot also the training points\n",
        "    y = np.unique(y_train)\n",
        "    for k in range(ncls):\n",
        "        ax.scatter(X_train[y_train==y[k], 0], X_train[y_train==y[k], 1], c=colors[k%nc],  marker=markers[k%nm], cmap=cmap[k%ncmap], edgecolors='k', label='Training data', alpha=1)\n",
        "    # and testing points if given\n",
        "    if X_test is not None and y_test is not None:\n",
        "        y = np.unique(y_train)\n",
        "        for k in range(ncls):\n",
        "            ax.scatter(X_test[y_test==y[k], 0], X_test[y_test==y[k], 1], c=colors[k],  marker=markers[k], cmap=cmap[k], edgecolors='k', label='Test data', alpha=1)\n",
        "        plt.legend(loc=\"upper right\", fontsize=16, frameon=True)\n",
        "        ax.get_legend().legendHandles[0].set_color('k')\n",
        "        ax.get_legend().legendHandles[1].set_color('k')\n",
        "\n",
        "    ax.set_xlim(xx.min(), xx.max())\n",
        "    ax.set_ylim(yy.min(), yy.max())\n",
        "    plt.axis('tight')\n",
        "    plt.xlabel('x1', fontsize=16)\n",
        "    plt.ylabel('x2', fontsize=16)\n",
        "    plt.xticks(fontsize=16)\n",
        "    plt.yticks(fontsize=16)\n",
        "    plt.gca().set_aspect('equal')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('rnd2d_ex1_LDA.png', transparent=True,dpi=300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obFLzXN6G9Z_",
        "colab_type": "text"
      },
      "source": [
        "Make training data\n",
        "------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQWiGCU0G9aA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example 1': define manually\n",
        "X = np.array([[22, 1], [13,2], [19,5], [15,8], [11,10], [7,0]])\n",
        "y = np.array([1,1,1,1,-1,-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPcFoZwoU2fp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example 2: draw npos and nneg points from the Gaussian distribution for each class\n",
        "npos = 30\n",
        "nneg = 30\n",
        "np.random.seed(321)\n",
        "X = np.r_[np.random.randn(npos, 2) + [3, 3], np.random.randn(nneg, 2)]\n",
        "# [1,1,...,1,-1,-1,...,-1]\n",
        "y = np.array([1] * npos + [-1] * nneg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAcLs8fxU3b8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example 3: create moons using sklearn\n",
        "from sklearn.datasets import make_moons\n",
        "X, y = make_moons(n_samples=100, noise=0.2, random_state=0)\n",
        "y[y==0] = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5xj700RhOad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example 4: create circles using sklearn\n",
        "from sklearn.datasets import make_circles\n",
        "X, y = make_circles(n_samples=150, noise=0.1, random_state=0, factor=0.3)\n",
        "y[y==0] = -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbqXssHcU5Is",
        "colab_type": "text"
      },
      "source": [
        "Plot the training points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl-028CoG9aE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the training points\n",
        "ax = plt.figure()\n",
        "ax = plt.axes()\n",
        "ax.scatter(X[y>0, 0], X[y>0, 1], c='r',  marker='s', cmap=plt.cm.bwr, edgecolors='k', label='Training data', alpha=1)\n",
        "ax.scatter(X[y<=0, 0], X[y<=0, 1], c='b', marker='o', cmap=plt.cm.bwr, edgecolors='k', label='Training data', alpha=1)\n",
        "plt.xlabel('x1', fontsize=16)\n",
        "plt.ylabel('x2', fontsize=16)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.gca().set_aspect('equal')\n",
        "ax.set_xlim(X[:,0].min()-0.5, X[:,0].max()+0.5)\n",
        "ax.set_ylim(X[:,1].min()-0.5, X[:,1].max()+0.5)\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT0P8z26G9aT",
        "colab_type": "text"
      },
      "source": [
        "Run the training\n",
        "----------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA7s-t0MG9ae",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Linear discriminant analysis\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "model = LinearDiscriminantAnalysis(store_covariance=True)\n",
        "model.fit(X,y)\n",
        "\n",
        "#number of classes\n",
        "print(\"# of classes: \", len(model.classes_))\n",
        "\n",
        "#probability of each class\n",
        "print(\"Prior probs: \", model.priors_)\n",
        "\n",
        "#mean and variance of each feature per class\n",
        "print(\"Mean: \", model.means_)\n",
        "print(\"Variance: \", model.covariance_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0QWpXIs_ngb",
        "colab_type": "text"
      },
      "source": [
        "Visualize the Gaussian distributions\n",
        "-----------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pok0V1O2jk1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dlim = np.sqrt(np.diag(model.covariance_))\n",
        "plot2d_LDA(model, X, y, dxlim=dlim[0], dylim=dlim[1], markers=['x','o'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0NGCvSX8mKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classification test\n",
        "Xt = np.array([[16,6]])\n",
        "predict_proba = model.predict_proba(Xt)\n",
        "print(predict_proba)\n",
        "print(\"Xt = \", \"is classified into\", model.predict(Xt), \"with probability\", np.max(predict_proba))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz-3aGkzOFxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# normal vector w and a point c\n",
        "w = np.linalg.inv(model.covariance_).dot(model.means_[1,:]-model.means_[0,:])\n",
        "c = 0.5*(model.means_[1,:]+model.means_[0,:])-(np.log(4./2.))/(model.means_[1,:]-model.means_[0,:]).dot(w)*(model.means_[1,:]-model.means_[0,:])\n",
        "print(\"w = \", w)\n",
        "print(\"c = \", c)\n",
        "\n",
        "# classifier\n",
        "g = lambda x: w.dot(x-c)\n",
        "print(g(Xt[0,:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUVP4jZM3i1y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"rnd2d_ex1_LDA.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}