{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "univariate_regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsakailab/prml/blob/master/ipynb/univariate_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TNrUFuuP0gmv",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bbHBDDmE0gmy"
      },
      "source": [
        "Univariate regression\n",
        "=================\n",
        "Fit a line or a polynome of specified degree to a 1d dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UoRFQUlLokm4"
      },
      "source": [
        "Make training data\n",
        "---------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_nCKZV9u0gmz",
        "colab": {}
      },
      "source": [
        "# Example 1: define manually\n",
        "import numpy as np\n",
        "x = np.array([5., 3., 0., 4.])\n",
        "y = np.array([4., 4., 1., 3.])\n",
        "x_test = np.array([1.0,  2., 2.5, 4.])\n",
        "y_test = np.array([1.5, 2., 3.5, 4.])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HIJqBgJ2pPyZ",
        "colab": {}
      },
      "source": [
        "# Example 2: the diabetes dataset\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "diabetes = datasets.load_diabetes()\n",
        "diabetes.target /= diabetes.target.max()\n",
        "\n",
        "# Use only one feature\n",
        "x = diabetes.data[-20:, np.newaxis, 2].ravel()\n",
        "y = diabetes.target[-20:]\n",
        "x_test = diabetes.data[:-20, np.newaxis, 2].ravel()\n",
        "y_test = diabetes.target[:-20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZvxP6Rlniu6J",
        "colab": {}
      },
      "source": [
        "# Example 3: artificial dataset\n",
        "# http://www.scipy-lectures.org/packages/scikit-learn/auto_examples/plot_bias_variance.html#bias-and-variance-of-polynomial-fit\n",
        "import numpy as np\n",
        "generating_func = lambda x, err=0.5: np.random.normal(10 - 1. / (x + 0.1), err)\n",
        "n_samples = 20\n",
        "np.random.seed(0)\n",
        "x = 10 ** np.linspace(-2, 0, n_samples)\n",
        "y = generating_func(x)\n",
        "x_test = np.linspace(-0.2, 1.2, 100)\n",
        "y_test = generating_func(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S3Ah1akFow7B"
      },
      "source": [
        "plot the training points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R1SAIZS7BCqa",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(3, 3.5))\n",
        "ax = plt.axes()\n",
        "ax.scatter(x_test, y_test, marker='o', c='0.75', s=20)\n",
        "ax.scatter(x, y, marker='x', c='b', s=50)\n",
        "xmin, xmax, dx = x.min(), x.max(), (x.max()-x.min())*0.2\n",
        "ymin, ymax, dy = y.min(), y.max(), (y.max()-y.min())*0.2\n",
        "ax.set_xlim(xmin-dx, xmax+dx)\n",
        "ax.set_ylim(ymin-dy, ymax+dy)\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4UTrkDQEu_EU"
      },
      "source": [
        "Compute the model parameters\n",
        "-------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "plfSF04aeFWL",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "degree = 1\n",
        "X = PolynomialFeatures(degree).fit_transform(x[:, np.newaxis])\n",
        "if degree < 4:\n",
        "    print(\"X =\");    print(X)\n",
        "    print(\"X.T.dot(X) =\");    print(X.T.dot(X))\n",
        "w = np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))\n",
        "print(\"w = X.T.dot(X)).dot(X.T.dot(y) =\", w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tgg8IB2whMHQ",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(3, 3.5))\n",
        "ax = plt.axes()\n",
        "x_regr = np.linspace(xmin-dx, xmax+dx, 100)\n",
        "X_regr = PolynomialFeatures(degree).fit_transform(x_regr[:, np.newaxis])\n",
        "y_regr = X_regr.dot(w)\n",
        "ax.plot(x_regr, y_regr, 'b-')\n",
        "ax.scatter(x_test, y_test, marker='o', c='0.75', s=20)\n",
        "ax.scatter(x, y, marker='x', c='b', s=50)\n",
        "ax.set_xlim(xmin-dx, xmax+dx)\n",
        "ax.set_ylim(ymin-dy, ymax+dy)\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('univariate_regression.png', transparent=True,dpi=300)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "print(\"l2 loss (residual) = %.2f\" % mean_squared_error(y, X.dot(w)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fPbdEvDtw_74"
      },
      "source": [
        "Validation of degrees\n",
        "-------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eRPDDkcju5VS",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(3, 3.5))\n",
        "ax = plt.axes()\n",
        "ax.scatter(x_test, y_test, marker='o', c='0.75', s=20)\n",
        "\n",
        "dmax = min(len(y), 16)\n",
        "residual = np.zeros(dmax)\n",
        "mse_test = np.zeros(dmax)\n",
        "for degree in range(dmax):\n",
        "    # regression\n",
        "    X = PolynomialFeatures(degree).fit_transform(x[:, np.newaxis])\n",
        "    w = np.linalg.inv(X.T.dot(X)).dot(X.T.dot(y))\n",
        "\n",
        "    # compute the mean residual of training data and the mean square error of test data\n",
        "    residual[degree] = mean_squared_error(y, X.dot(w))\n",
        "    X_test = PolynomialFeatures(degree).fit_transform(x_test[:, np.newaxis])\n",
        "    mse_test[degree] = mean_squared_error(y_test, X_test.dot(w))\n",
        "\n",
        "    # plot the regression curve\n",
        "    if degree in [1, 2, 3, 5, 7, 11]:\n",
        "        x_regr = np.linspace(xmin-dx, xmax+dx, 100)\n",
        "        X_regr = PolynomialFeatures(degree).fit_transform(x_regr[:, np.newaxis])\n",
        "        y_regr = X_regr.dot(w)\n",
        "        alpha = min(1, 2**mse_test[0]/mse_test[degree])\n",
        "        val = min(1, 2**residual[degree]/residual[0])\n",
        "        ax.plot(x_regr, y_regr,  lw=1, color =[1-val,1-val,1], alpha=alpha)\n",
        "\n",
        "ax.scatter(x, y, marker='x', c='b', s=50)\n",
        "ax.set_xlim(xmin-dx, xmax+dx)\n",
        "ax.set_ylim(ymin-dy, ymax+dy)\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "plt.tight_layout()\n",
        "plt.savefig('polynomial_regression.png', transparent=True,dpi=300)\n",
        "\n",
        "plt.figure()\n",
        "ax = plt.axes()\n",
        "if mse_test.max() - mse_test.min() < 1e+2:\n",
        "    ax.plot(range(dmax), residual, 'bs--', label='Train')\n",
        "    ax.plot(range(dmax), mse_test, 'ko--', label='Test')\n",
        "else:\n",
        "    ax.semilogy(range(dmax), residual, 'bs--', label='Train')\n",
        "    ax.semilogy(range(dmax), mse_test, 'ko--', label='Test')\n",
        "plt.axis('tight')\n",
        "plt.xlabel('Degree', fontsize=20)\n",
        "plt.ylabel('$\\\\ell_2$ loss (residual)', fontsize=20)\n",
        "plt.legend(loc=\"upper right\", fontsize=16, frameon=True)\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "plt.xticks(fontsize=20)\n",
        "plt.yticks(fontsize=20)\n",
        "ax.set_xlim(-0.1,dmax-0.9)\n",
        "plt.tight_layout()\n",
        "plt.savefig('univariate_regression_loss_vs_degree.png', transparent=True,dpi=300)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}