{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsakailab/prml/blob/master/ipynb/ex_MNIST_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PCAによる低次元化（dimensionality reduction by principal component analysis）\n",
        "\n",
        "主成分分析（PCA）でデータを低次元空間に埋め込む圧縮，再構成による近似の性質を観察しましょう．\n",
        "\n",
        "参考資料：\n",
        "- [Principal component analysis@Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis)\n",
        "- [Principal component analsys@scikit-learn](https://scikit-learn.org/stable/modules/decomposition.html#pca)\n",
        "\n",
        "----\n",
        "\n",
        "氏名：\n",
        "\n",
        "学生番号：\n",
        "\n",
        "----\n",
        "## 基本課題（必須）\n",
        "\n",
        "    1. Fashion-MNISTの 2，4，6，8 の 4 クラスからなる画像集合を主成分分析で低次元化したとき，\n",
        "       上位の主成分はどのような画像の違いを表現していますか．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "\n",
        "    2. Fashion-MNISTの 2，4，6，8 の 4 クラスからなる画像集合を上位の主成分を用いて圧縮・再構成したとき，\n",
        "       画像のどのような特徴が主に再構成され易い・され難いですか．また，その原因を考えてください．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "\n",
        "    3. 再構成誤差は，主成分の数に対してどのように依存しますか．特に，主成分の数が小さ過ぎる・大き過ぎるとき，\n",
        "       どのような原因で訓練画像の再構成誤差が大きく・小さくなると考えられますか．\n",
        "       ノイズの大小や訓練データ数による違いも踏まえて考察してください．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "\n",
        "    4.その他，気づいたこと，調べたことを書いてください．\n",
        "\n",
        "（ここに回答を書いてください）\n",
        "\n",
        "\n",
        "----\n",
        "発展課題（任意）がこのノートブックの後半にあります．"
      ],
      "metadata": {
        "id": "AH8-0BiKPDuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### データ集合を取得します．"
      ],
      "metadata": {
        "id": "tC7fn8ijeihu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "# [MNIST](http://yann.lecun.com/exdb/mnist/)\n",
        "#tvds = datasets.MNIST('./data', download=True, train=True)\n",
        "\n",
        "# [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)\n",
        "tvds = datasets.FashionMNIST('./data', download=True, train=True)\n",
        "\n",
        "# [Kuzushiji-MNIST](https://github.com/rois-codh/kmnist)\n",
        "#tvds = datasets.KMNIST('./data', download=True, train=True)\n",
        "\n",
        "Ximages_all, y_all = tvds.data.numpy(), tvds.targets.numpy()\n",
        "\n",
        "# [EMNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.EMNIST.html)\n",
        "#tvds = datasets.EMNIST('./data', download=True, train=True, split='letters')\n",
        "#Ximages_all, y_all = tvds.data.transpose_(-1,-2).numpy(), tvds.targets.numpy() - 1"
      ],
      "metadata": {
        "id": "pBKK4QZN1eCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ☆画像数とサイズを確認します（画像を加工したい場合はこのセルを編集して利用してください）．\n",
        "\n",
        "# simulate noisy images\n",
        "#Ximages_all = Ximages_all + np.random.rand(*Ximages_all.shape) * 200\n",
        "#Ximages_all[Ximages_all > 255] = 255\n",
        "\n",
        "import numpy as np\n",
        "from skimage.filters import gaussian\n",
        "from skimage.exposure import equalize_hist as eh\n",
        "from skimage.transform import resize\n",
        "\n",
        "'''\n",
        "* blurring (https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.gaussian)\n",
        "* histogram equalization (https://scikit-image.org/docs/stable/auto_examples/color_exposure/plot_equalize.html)\n",
        "* resize images (https://scikit-image.org/docs/stable/auto_examples/transform/plot_rescale.html)\n",
        "'''\n",
        "#Ximages_all = gaussian(np.float32(Ximages_all.transpose((1,2,0))), sigma=1.0, multichannel=True).transpose(2,0,1)\n",
        "#Ximages_all = eh(Ximages_all.transpose((1,2,0))).transpose(2,0,1) * 255\n",
        "#height, width = 8, 8; Ximages_all = resize(Ximages_all.transpose((1,2,0)), (height, width)).transpose(2,0,1)\n",
        "\n",
        "Ximages_all = np.uint8(Ximages_all)\n",
        "print(\"(#images, height, width)\", Ximages_all.shape)\n",
        "print(Ximages_all.dtype, \", max. pixel value = \", Ximages_all.max())"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9w91jXrgIKuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST class labels\n",
        "\n",
        "| [MNIST](http://yann.lecun.com/exdb/mnist/) | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |\n",
        "| - | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
        "| [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) | T-shirt/top | Trouser | Pullover | Dress | Coat | Sandal |  Shirt | Sneaker | Bag | Ankle boot |\n",
        "| [Kuzushiji-MNIST](https://github.com/rois-codh/kmnist) | お | き | す | つ | な | は |  ま | や | れ | を |"
      ],
      "metadata": {
        "id": "b85HjGXpQuO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 画像を表示する関数 implot を定義し，画像を例示します．\n",
        "#@title Show images of digits\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.colors import TwoSlopeNorm as tsn\n",
        "import inspect\n",
        "\n",
        "# show the digits (imgs.shape=(n,h*w) or (n,h,w) or (n,h,w,c))\n",
        "def implot(imgs, labels=None, preds=None, num=8, mag=0.6, shape=None, vmin=None, vmax=None, cmap=None):\n",
        "    gts = np.ones(len(imgs)) if labels is None else labels\n",
        "    classes = np.unique(gts)\n",
        "    nc = len(classes)\n",
        "    idx = np.arange(len(gts))\n",
        "    if imgs.ndim == 2 and shape is None:\n",
        "        print(\"Set the shape, e.g.,\" + inspect.currentframe().f_code.co_name + \"(images, shape=(h,w))\")\n",
        "        return\n",
        "    norm = None\n",
        "    if imgs.ndim <= 3 and cmap is None:\n",
        "        minimgs, maximgs = imgs.min(), imgs.max()\n",
        "        if minimgs < 0:\n",
        "            norm = tsn(vmin=np.minimum(minimgs,-1e-6), vcenter=0, vmax=np.maximum(maximgs,1e-6))\n",
        "            cmap = plt.cm.seismic\n",
        "        else:\n",
        "            cmap = plt.cm.gray\n",
        "    fig = plt.figure(figsize=(num*mag, nc*mag))  # figure size in inches\n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "    for i, c in enumerate(classes):\n",
        "        idc = idx[gts==c]\n",
        "        imgsc = imgs[gts==c]\n",
        "        if imgsc.ndim == 2:\n",
        "            imgsc = imgsc.reshape(-1, *shape)\n",
        "        for j in range(min(num,len(imgsc))):\n",
        "            ax = fig.add_subplot(nc, num, i*num+j+1, xticks=[], yticks=[])\n",
        "            ax.imshow(imgsc[j].reshape(shape), cmap=cmap, interpolation='nearest', vmin=vmin, vmax=vmax, norm=norm)\n",
        "\n",
        "            # label the image with the target value\n",
        "            if labels is not None:\n",
        "                ax.text(0, imgs.shape[1]*0.2, str(c), color='white')\n",
        "            if preds is not None:\n",
        "                if preds[idc[j]] == c:\n",
        "                    ax.text(imgs.shape[2]*0.8, imgs.shape[1]*0.2, str(preds[idc[j]]), color='#a0ffa0')\n",
        "                else:\n",
        "                    ax.text(imgs.shape[2]*0.8, imgs.shape[1]*0.2, str(preds[idc[j]]), color='red')\n",
        "\n",
        "print(\"%d images in total\" % len(y_all))\n",
        "p = np.random.permutation(len(y_all))\n",
        "implot(Ximages_all[p], y_all[p], num=16, vmin=0, vmax=255)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ivCIQfztnbuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76r4Ic8VAj_g"
      },
      "source": [
        "### クラスを抜粋したい場合（さもなくば実行しなくてよいです）\n",
        "- `classes` でクラスを指定します．例：`classes = [1,4,7,9]`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVowqB1Z_Pcs"
      },
      "source": [
        "classes = [0,2,4,6]  # choose from 0 to 9\n",
        "\n",
        "classes = np.unique(classes)\n",
        "is_in_classes = list(map(lambda c: y_all == c, classes))\n",
        "Ximages = Ximages_all[np.logical_or.reduce(is_in_classes)]\n",
        "y = y_all[np.logical_or.reduce(is_in_classes)]\n",
        "\n",
        "p = np.random.permutation(len(y))\n",
        "implot(Ximages[p], y[p], num=20, vmin=0, vmax=255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vc7EF1HS_QMb"
      },
      "source": [
        "### すべてのクラスを使いたい場合"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohWRmfaW_QMc"
      },
      "source": [
        "Ximages = Ximages_all\n",
        "y = y_all\n",
        "classes = np.unique(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ☆訓練データと検証データに分けます．\n",
        "- 画像を `Ximages_train` と `Ximages_val` に分けます．それぞれ `n_train` 枚と `n_val` 枚の画像です．\n",
        "- `Ximages_train` と `Ximages_val` を，それぞれ shape が `(n_train, 画素数)`，`(n_val，画素数)` の NumPy 配列にしたものが `X_train`，`X_val` です．"
      ],
      "metadata": {
        "id": "H9BCD7UEZxet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_size = 0.8\n",
        "\n",
        "# split the data into training and validation sets\n",
        "Ximages_train, Ximages_val, y_train, y_val = train_test_split(Ximages, y, train_size=train_size)\n",
        "\n",
        "n_train = len(Ximages_train)\n",
        "n_val = len(Ximages_val)\n",
        "\n",
        "# reshape to 28*28=784-dimensional feature vectors\n",
        "X_train = np.reshape(Ximages_train, (Ximages_train.shape[0], -1))   # (n_train, 784)\n",
        "X_val = np.reshape(Ximages_val, (Ximages_val.shape[0], -1))         # (n_val, 784)\n",
        "print(\"X_train.shape = \", X_train.shape)\n",
        "print(\"X_val.shape = \", X_val.shape)"
      ],
      "metadata": {
        "id": "Zxtu4nWQZvtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ★訓練データに対して主成分分析を実行します．\n",
        "- [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)を使用して，標準化の前処理を施します．`X_train` を標準化したものが `Xs_train` です．この標準化に従って `X_val` を標準化したものが `Xs_val` です．\n",
        "- [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)を使用します．引数（特に`n_components`）の仕様とデフォルトの値を確認してください．\n",
        "- `model.fit` は，標本平均 $\\boldsymbol{\\mu}$ と，$k=$ `n_components` 本の主成分（分散共分散行列の固有ベクトル）$\\boldsymbol{U}_{:k}$ を計算します．\n",
        "- クラスラベル `y_train` と `y_val` は主成分分析に必要ありません．"
      ],
      "metadata": {
        "id": "3cy3uyS35LYw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "Xs_train = scaler.fit_transform(X_train)\n",
        "Xs_val = scaler.transform(X_val)\n",
        "\n",
        "n_components=100\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "model = PCA(n_components=n_components)\n",
        "model.fit(Xs_train)"
      ],
      "metadata": {
        "id": "7-A2ITIiuqQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 標本平均 $\\boldsymbol{\\mu}$ および主成分 $\\boldsymbol{U}_{:k}$ を可視化します．\n",
        "- 標本平均は `model.mean_` です．標準化から0～255の画素値に戻して表示します．\n",
        "- 正を赤，負を青の画素値とする画像として各主成分を表示します．"
      ],
      "metadata": {
        "id": "a7EOHIGHJYKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h, w = Ximages.shape[1],Ximages.shape[2]\n",
        "implot(scaler.inverse_transform(model.mean_.reshape(1,-1)).reshape(-1,h,w), vmin=0, vmax=255)\n",
        "implot(model.components_.reshape(-1,h,w), num=min(20,n_components))"
      ],
      "metadata": {
        "id": "l62M_UeE14oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 主成分分析でデータを低次元化します．\n",
        "- `Xs_train` と `Xs_val` を低次元化したものが `Z_train` と `Z_val` です．`model.transform` はデータ $\\boldsymbol x$ から $k$ 次元ベクトル $\\boldsymbol z$ を次式で計算します．\n",
        "$\\boldsymbol{z}=\\boldsymbol{U}_{:k}^\\top(\\boldsymbol{x}-\\boldsymbol{\\mu})$"
      ],
      "metadata": {
        "id": "wJ5zQGQTgMDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Z_train = model.transform(Xs_train)\n",
        "Z_val = model.transform(Xs_val)"
      ],
      "metadata": {
        "id": "A9x3n9YUwW7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title `Z_train` と `Z_val` の第1，2，3成分を，ラベル `y` で色分けしてプロットします．<p><ul><li>プロットする関数 `plot_embedding3d` を定義します．</li></ul></p>\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "#from matplotlib import offsetbox\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import plotly.graph_objs  as go\n",
        "import plotly.graph_objs.layout  as gol\n",
        "#import plotly.io as pio\n",
        "#pio.renderers.default = 'colab'\n",
        "\n",
        "def plot_embedding3d(X, y, xlim=None, ylim=None, zlim=None, Scaler=None, cmap=plt.cm.tab10):\n",
        "    if Scaler is not None:\n",
        "        X3d = Scaler.fit_transform(X[:,:3])\n",
        "        if xlim is None: xlim = [-3.1, 3.1]\n",
        "        if ylim is None: ylim = [-3.1, 3.1]\n",
        "        if zlim is None: zlim = [-3.1, 3.1]\n",
        "    else:\n",
        "        X3d = X[:,:3]\n",
        "\n",
        "    # https://plotly.com/python-api-reference/generated/plotly.graph_objects.Scatter3d.html#plotly.graph_objects.Scatter3d\n",
        "    trace = go.Scatter3d(x=X3d[:,0], y=X3d[:,1], z=X3d[:,2], mode='text',\n",
        "                         text=y, textfont=dict(color=['rgba({},{},{},{})'.format(c[0],c[1],c[2],0.8) for c in cmap(y)]), textposition='top center'\n",
        "    )\n",
        "\n",
        "    layout = go.Layout(margin=dict(l=0,r=0,b=0,t=0), scene=gol.Scene(aspectmode='cube', xaxis=gol.scene.XAxis(title=\"comp. 1\"), yaxis=gol.scene.YAxis(title=\"comp. 2\"), zaxis=gol.scene.ZAxis(title=\"comp. 3\")))\n",
        "    fig = go.Figure(data=[trace], layout=layout)\n",
        "    camera = dict(up=dict(x=0, y=0, z=3), center=dict(x=0, y=0, z=0), eye=dict(x=1.5, y=1.5, z=0.8))\n",
        "    scene = dict(xaxis=dict(range=xlim), yaxis=dict(range=ylim), zaxis=dict(range=zlim))\n",
        "    fig.update_layout(scene_camera=camera, scene=scene)\n",
        "    fig.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VF1Cwp-iWDZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 全部表示すると多いので，ランダムに n 個だけ表示する．\n",
        "nt = min(300, n_train)\n",
        "pt = np.random.choice(X_train.shape[0], nt, replace=False)\n",
        "plot_embedding3d(Z_train[pt], y_train[pt])\n",
        "\n",
        "nv = min(300, n_val)\n",
        "pv = np.random.choice(Z_val.shape[0], nv, replace=False)\n",
        "plot_embedding3d(Z_val[pv], y_val[pv])"
      ],
      "metadata": {
        "id": "8R0hwrK2WyYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MNIST class labels\n",
        "\n",
        "| [MNIST](http://yann.lecun.com/exdb/mnist/) | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |\n",
        "| - | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: | :-: |\n",
        "| [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) | T-shirt/top | Trouser | Pullover | Dress | Coat | Sandal |  Shirt | Sneaker | Bag | Ankle boot |\n",
        "| [Kuzushiji-MNIST](https://github.com/rois-codh/kmnist) | お | き | す | つ | な | は |  ま | や | れ | を |"
      ],
      "metadata": {
        "id": "yukS41WxzDLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 主成分の数 $k$ とノイズに対する再構成の依存性を調べます．\n",
        "- `X_train` と `X_val` にノイズを加えたデータを `Xn_train` ， `Xn_val` とします．\n",
        "- `Xn_train` と `Xn_val` を標準化後，主成分分析で `Z_train` と `Z_val` に低次元化します．\n",
        "- $\\hat{\\boldsymbol{x}}=\\boldsymbol{U}_{:k}\\boldsymbol{z}_{:k}+\\boldsymbol{\\mu}$ を計算して返す関数 `inverse_transform` を定義します．`k=n_components` のとき，`inverse_transform(model, Z, n_components)` は [`model.inverse_transform(Z)`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.inverse_transform) と同じです．\n",
        "- `k` を指定して再構成し，ノイズを加えていない元画像と再構成画像を比較します．"
      ],
      "metadata": {
        "id": "SaLIf_PB5mgE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add noise\n",
        "noise_std = 50 # [0,255]\n",
        "Xn_train = X_train + np.random.randn(*X_train.shape) * noise_std\n",
        "Xn_val = X_val + np.random.randn(*X_val.shape) * noise_std"
      ],
      "metadata": {
        "id": "O3kxBOcEWG_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "Xs_train = scaler.fit_transform(Xn_train)\n",
        "Xs_val = scaler.transform(Xn_val)\n",
        "\n",
        "n_components=300\n",
        "from sklearn.decomposition import PCA\n",
        "model = PCA(n_components=n_components, svd_solver='full').fit(Xs_train)\n",
        "\n",
        "Z_train = model.transform(Xs_train)\n",
        "Z_val = model.transform(Xs_val)"
      ],
      "metadata": {
        "id": "DRVb1Y5G9AyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inverse_transform(model, Z, k):\n",
        "    UkT = model.components_[:k]\n",
        "    mu = model.mean_\n",
        "    Xr = Z[:,:k].dot(UkT) + mu\n",
        "    return Xr\n",
        "\n",
        "def show_reconst(model, Z, k, X, scaler, num=20):\n",
        "    Xr = inverse_transform(model, Z, k)\n",
        "    Xr = scaler.inverse_transform(Xr)\n",
        "    implot(Xr.reshape(-1,h,w), cmap=plt.cm.gray, vmin=0, vmax=255, num=num)\n",
        "    implot((Xr - X).reshape(-1,h,w), vmin=-255,vmax=255, num=num)"
      ],
      "metadata": {
        "id": "Pwt7NPHo8ec2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5   # must be no greater than n_components\n",
        "\n",
        "num = 20\n",
        "print(\"Train: original (1st row), original with noise (2nd row), reconstructed (3rd row), and error [-255(blue),255(red)] (4th row).\")\n",
        "implot(X_train.reshape(-1,h,w), cmap=plt.cm.gray, vmin=0, vmax=255, num=num)\n",
        "implot(Xn_train.reshape(-1,h,w), cmap=plt.cm.gray, vmin=0, vmax=255, num=num)\n",
        "show_reconst(model, Z_train, k, X_train, scaler, num=num)\n",
        "\n",
        "plt.show();print(\"\\n\")\n",
        "\n",
        "print(\"Val: original (1st row), original with noise (2nd row), reconstructed (3rd row), and error [-255(blue),255(red)] (4th row).\")\n",
        "implot(X_val.reshape(-1,h,w), cmap=plt.cm.gray, vmin=0, vmax=255, num=num)\n",
        "implot(Xn_val.reshape(-1,h,w), cmap=plt.cm.gray, vmin=0, vmax=255, num=num)\n",
        "show_reconst(model, Z_val, k, X_val, scaler, num=num)"
      ],
      "metadata": {
        "id": "Cng0cvIY_XAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### $k$ に対する再構成誤差を調べます．"
      ],
      "metadata": {
        "id": "DDeYH17DgsA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_error(model, Z, k, X, scaler):\n",
        "    Xr = inverse_transform(model, Z, k)\n",
        "    Xr = scaler.inverse_transform(Xr)\n",
        "    Err = Xr - X\n",
        "    mse = np.sqrt(np.mean(Err**2))\n",
        "    std = np.std(Err)\n",
        "    return mse, std\n",
        "\n",
        "k_components = np.arange(5, n_components, 10)\n",
        "MSE_train, MSE_val = [], []\n",
        "for k in k_components:\n",
        "    mse, _ = compute_error(model, Z_train, k, X_train, scaler)\n",
        "    MSE_train.append(mse)\n",
        "    mse, _ = compute_error(model, Z_val, k, X_val, scaler)\n",
        "    MSE_val.append(mse)\n",
        "\n",
        "plt.plot(k_components, MSE_train, '.', label=\"train\")\n",
        "plt.plot(k_components, MSE_val, '+', label=\"val\")\n",
        "plt.ylim([0,None])\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "3URduAl70i6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------\n",
        "## 発展課題（任意）\n",
        "\n",
        "    1. PCAを実装してください．\n",
        "\n",
        "（回答は「★PCAの実装」に書いてください）\n",
        "\n",
        "    2. 特異値分解（singular value decomposition; SVD）について調査し，\n",
        "       標本分散共分散行列の固有ベクトルが SVD で計算できることを説明してください．\n",
        "\n",
        "（ここに回答を書いてください）"
      ],
      "metadata": {
        "id": "2QsruOf_mYxr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ★PCAの実装\n",
        "\n",
        "`myPCA`という名のクラスとして実装しましょう．\n",
        "[sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) に似た仕様とします．\n",
        "\n",
        "訓練データ `X_train` から主成分を計算したあと，`X_val` を低次元化するには，\n",
        "```\n",
        "    model = myPCA(n_estimators=300)\n",
        "    model.fit(X_train)\n",
        "    Z_val = model.transform(X_val)\n",
        "```\n",
        "のように使うことを想定します．\n",
        "- 訓練データの次元数`d`，個数`n`の場合，`X_train` は shape が `(n,d)` のNumPy配列です．\n",
        "- `myPCA` の引数 `n_components` は主成分（固有ベクトル）の数です．"
      ],
      "metadata": {
        "id": "cwFEAhOawTvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import svd\n",
        "\n",
        "class myPCA:\n",
        "\n",
        "    def __init__(self, n_components):\n",
        "        self.n_components = n_components\n",
        "        self.components_ = None\n",
        "        self.mean_ = None\n",
        "        self.singular_values_ = None\n",
        "\n",
        "    def fit(self, X):                                       # X(n,d)\n",
        "        # X の標本平均を計算します．\n",
        "        self.mean_ = ''' np.mean() を使う '''\n",
        "\n",
        "        # 標本分散共分散行列の固有ベクトルを得ます．\n",
        "        _, sv, ev = svd(X - self.mean_, full_matrices=False)\n",
        "        self.components_ = ev[0:self.n_components]\n",
        "        self.singular_values_ = sv[0:self.n_components]     # (n_components,d)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        if self.mean_ is not None:\n",
        "            X_transformed = ''' self.mean_ と self.components を用いて X を低次元化します '''\n",
        "        return X_transformed"
      ],
      "metadata": {
        "id": "rhVKAwqrzT8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "お疲れさまでした．"
      ],
      "metadata": {
        "id": "jBz0cgBO0qZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 付録：[MedMNIST](https://github.com/MedMNIST/MedMNIST)を使う場合<p>以下を実行後，このJupyter Notebook前半の「☆画像数とサイズを確認します（画像を加工したい場合はこのセルを編集して利用してください）．」以降を実行できます．\n",
        "!pip install -q medmnist\n",
        "import medmnist\n",
        "print(f\"MedMNIST v{medmnist.__version__} @ {medmnist.HOMEPAGE}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "19n5ZTkG1eH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 'pneumoniamnist', 'chestmnist', 'octmnist', 'breastmnist', 'tissuemnist', 'organamnist', 'organcmnist', 'organsmnist' \n",
        "data_flag = 'pneumoniamnist' \n",
        "DataClass = getattr(medmnist, medmnist.INFO[data_flag]['python_class'])\n",
        "\n",
        "tvds = DataClass(split='train', download=True)\n",
        "#tvds = DataClass(split='test', download=True)\n",
        "\n",
        "print(tvds)\n",
        "Ximages_all, y_all = tvds.imgs, tvds.labels.ravel()"
      ],
      "metadata": {
        "id": "2JWf5Htq1eLD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}