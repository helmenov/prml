{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2Xf5xoadG9Z0"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UW1hl94VG9Z8"
   },
   "source": [
    "Logistic regression of a 2D data set\n",
    "==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PCg8jJ2AG9aH"
   },
   "source": [
    "Define functions\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NJxRr0_4G9aI"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Linear descriminant function\n",
    "def g(X, w):\n",
    "    return w[0] + np.dot(X, w[1:])\n",
    "\n",
    "# Logistic function\n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EaibuLlOG9aL"
   },
   "outputs": [],
   "source": [
    "# Logistic loss function\n",
    "def loss_function(X, y, w):    # X(n,d), y(n,), w(d+1,)\n",
    "    n = len(y)\n",
    "    a = sigmoid(g(X, w))\n",
    "    return np.where( y > 0, - np.log(a), -np.log(1-a) ).sum() / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "H_oYcRCTG9aO"
   },
   "outputs": [],
   "source": [
    "# Gradient descent (GD) algorithm\n",
    "def update_weights(X, y, w, lr):    # X(n,d), y(n,), w(d+1,)\n",
    "\n",
    "    n, d = X.shape\n",
    "    z = g(X, w)\n",
    "    a = sigmoid(z)\n",
    "    e = np.where(y > 0, a-1, a)     # e = y>0 ? a-1: a;\n",
    "\n",
    "    # Compute the gradient of the logistic loss function\n",
    "    delta = np.zeros(d+1)\n",
    "    delta[0] = e.sum()\n",
    "    delta[1:] = np.dot(e, X)\n",
    "    \n",
    "    # Update by subtraction\n",
    "    w -= lr * delta / n\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kA1AL6EWG9aR"
   },
   "outputs": [],
   "source": [
    "# Training by GD for a given training dataset (X_train, y_train)\n",
    "def train(X_train, y_train, lr=1.0, w=None, iters=100):\n",
    "\n",
    "    d = X_train.shape[1]\n",
    "    if w is None:\n",
    "        w = np.zeros(d+1)\n",
    "\n",
    "    cost_history = []\n",
    "    every = iters // 10\n",
    "    for i in range(iters):\n",
    "        w = update_weights(X_train, y_train, w, lr)\n",
    "\n",
    "        # Calculate error for auditing purposes\n",
    "        cost = loss_function(X_train, y_train, w)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Log Progress\n",
    "        if i % every == 0:\n",
    "            print(\"iter: \"+str(i) + \" cost: \"+str(cost) + str(w))\n",
    "    return w, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-PbAn1DPG9aX"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# Visualization of the decision boundary and regions\n",
    "def plot2d_classification(decision_function, X_train, y_train, X_test=None, y_test=None, w=None, cmap=plt.cm.bwr, xlim=None, ylim=None, levels=None, colors='k', linestyles=None):\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.axes()\n",
    "\n",
    "    if xlim is None:\n",
    "        xlim = [X_train[:, 0].min() - .5, X_train[:, 0].max() + .5]\n",
    "    if ylim is None:\n",
    "        ylim = [X_train[:, 1].min() - .5, X_train[:, 1].max() + .5]\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(xlim[0], xlim[1], 0.02), np.arange(ylim[0], ylim[1], 0.02))    \n",
    "\n",
    "    # Show prediction (P(y=+1 | X) by color by assigning a color to each point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    Z = decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    if levels is not None:\n",
    "        ax.contour(xx, yy, Z, levels=levels, colors=colors, linestyles=linestyles, alpha=0.5)\n",
    "    else:\n",
    "        ax.pcolor(xx, yy, Z, cmap=cmap, alpha=0.1, edgecolors=None)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    if w is not None:\n",
    "        x1 = np.linspace(xx.min(), xx.max(), 100)\n",
    "        if w[2] != 0:\n",
    "            x2 = -(w[0] + w[1] * x1) / w[2]\n",
    "            cnd = np.logical_and(x2<yy.max(), x2>yy.min())\n",
    "            plt.plot(x1[cnd], x2[cnd], 'k-')\n",
    "        else:\n",
    "            plt.axvline(x=-w[0]/w[1], color='k')\n",
    "\n",
    "    # Plot also the training points\n",
    "    ax.scatter(X_train[y_train>0, 0], X_train[y_train>0, 1], c='r',  marker='s', cmap=cmap, edgecolors='k', label='Training data', alpha=1)\n",
    "    ax.scatter(X_train[y_train<=0, 0], X_train[y_train<=0, 1], c='b', marker='o', cmap=cmap, edgecolors='k', label='Training data', alpha=1)\n",
    "    # and testing points if given\n",
    "    if X_test is not None and y_test is not None:\n",
    "        ax.scatter(X_test[y_test>0, 0], X_test[y_test>0, 1], c='r',  marker='s', cmap=cmap, edgecolors='k', label='Training data', alpha=1)\n",
    "        ax.scatter(X_test[y_test<=0, 0], X_test[y_test<=0, 1], c='b', marker='o', cmap=cmap, edgecolors='k', label='Training data', alpha=1)\n",
    "        plt.legend(loc=\"upper right\", fontsize=16, frameon=True)\n",
    "        ax.get_legend().legendHandles[0].set_color('k')\n",
    "        ax.get_legend().legendHandles[1].set_color('k')\n",
    "\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    plt.axis('tight')\n",
    "    plt.xlabel('x1', fontsize=16)\n",
    "    plt.ylabel('x2', fontsize=16)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.gca().set_aspect('equal')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7bCIjUckNOwF"
   },
   "outputs": [],
   "source": [
    "def  histogram_predict(decision_function, X_train, y_train, X_test=None, y_test=None, bins=None, normed=False):\n",
    "    if bins is None:\n",
    "        bins = len(y_train) // 4\n",
    "    plt.figure()\n",
    "    ax = plt.axes()\n",
    "    pred = decision_function(X_train)\n",
    "    plt.hist( [ pred[y_train>0], pred[y_train<=0] ], bins=bins, histtype='stepfilled', density=False, alpha=0.5, color=['r', 'b'], label=['$y=+1$', '$y=-1$'])\n",
    "    if X_test is not None and y_test is not None:\n",
    "        pred = decision_function(X_test)\n",
    "        plt.hist( [ pred[y_test>0], pred[y_test<=0] ], bins=bins, histtype='stepfilled', density=False, alpha=0.3, color=['r', 'b'], label=['$y_{test}=+1$', '$y_{test}=-1$'])\n",
    "    plt.xlabel(\"$g(x)$\", fontsize=16)\n",
    "    plt.ylabel(\"Frequency\", fontsize=16)\n",
    "    plt.legend(loc=\"upper right\", fontsize=16, frameon=True)\n",
    "    plt.axis('tight')\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    from matplotlib.ticker import FormatStrFormatter\n",
    "    plt.gca().yaxis.set_major_formatter(FormatStrFormatter('%1.0f'))\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "obFLzXN6G9Z_"
   },
   "source": [
    "Make training data\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "aQWiGCU0G9aA"
   },
   "outputs": [],
   "source": [
    "# Example 1: define manually\n",
    "X = np.array([[0, 0], [1,0], [0,1], [1,1]])\n",
    "y = np.array([-1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qPcFoZwoU2fp"
   },
   "outputs": [],
   "source": [
    "# Example 2: draw npos and nneg points from the Gaussian distribution for each class\n",
    "npos = 30\n",
    "nneg = 30\n",
    "np.random.seed(321)\n",
    "X = np.r_[np.random.randn(npos, 2) + [3, 3], np.random.randn(nneg, 2)]\n",
    "# [1,1,...,1,-1,-1,...,-1]\n",
    "y = np.array([1] * npos + [-1] * nneg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "IAcLs8fxU3b8"
   },
   "outputs": [],
   "source": [
    "# Example 3: create moons using sklearn\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, noise=0.2, random_state=0)\n",
    "y[y==0] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tbqXssHcU5Is"
   },
   "source": [
    "Plot the training points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cl-028CoG9aE"
   },
   "outputs": [],
   "source": [
    "# Plot the training points\n",
    "ax = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.scatter(X[y>0, 0], X[y>0, 1], c='r',  marker='s', cmap=plt.cm.bwr, edgecolors='k', label='Training data', alpha=1)\n",
    "ax.scatter(X[y<=0, 0], X[y<=0, 1], c='b', marker='o', cmap=plt.cm.bwr, edgecolors='k', label='Training data', alpha=1)\n",
    "plt.xlabel('x1', fontsize=16)\n",
    "plt.ylabel('x2', fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.gca().set_aspect('equal')\n",
    "ax.set_xlim(X[:,0].min()-0.5, X[:,0].max()+0.5)\n",
    "ax.set_ylim(X[:,1].min()-0.5, X[:,1].max()+0.5)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pT0P8z26G9aT"
   },
   "source": [
    "Run the training\n",
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NOrZbPLsG9aU"
   },
   "outputs": [],
   "source": [
    "w, ch = train(X, y, lr=1., iters=200, w=np.ones(3))\n",
    "print(w)\n",
    "plt.plot(ch)\n",
    "plt.xlabel(\"Iteration\", fontsize=16)\n",
    "plt.ylabel(\"Loss\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "LmEOwqiqG9ab"
   },
   "outputs": [],
   "source": [
    "# visualize sigmoid(g(X,w)) on training data (X,y)\n",
    "plot2d_classification(lambda X: sigmoid(g(X,w)), X, y,w=w)\n",
    "plt.savefig('rnd2d_ex1_logistic_regression.png', transparent=True)\n",
    "histogram_predict(lambda X: g(X,w), X, y)\n",
    "#histogram_predict(lambda X: predict(X,w), X, y)\n",
    "plt.savefig('hist_rnd2d_ex1_logistic_regression.png', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgYDQcqGG9ad"
   },
   "source": [
    "Logistic regression using scikit-learn\n",
    "--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OA7s-t0MG9ae"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X,y)\n",
    "\n",
    "w_skl = np.c_[model.intercept_, model.coef_].ravel()\n",
    "print(w_skl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "eIQAY-CgG9ah"
   },
   "outputs": [],
   "source": [
    "# visualize sigmoid(g(X,w)) by color\n",
    "plot2d_classification(lambda X: model.predict_proba(X)[:,1], X, y, w=w_skl)\n",
    "histogram_predict(lambda X: model.decision_function(X), X, y)\n",
    "#histogram_predict(lambda X: predict(X,w_skl), X, y)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "rnd2d_ex1_LogisticRegression_classification.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
